{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmEbtEtnEtQC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import sqlite3\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from category_encoders import OneHotEncoder\n",
        "from category_encoders import OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext sql\n",
        "%sql sqlite:////home/jovyan/nepal.sqlite"
      ],
      "metadata": {
        "id": "caJh_IYFEvNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT name\n",
        "FROM sqlite_schema\n",
        "WHERE type = \"table\""
      ],
      "metadata": {
        "id": "duYPbje-Ew7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT distinct(district_id)\n",
        "FROM id_map"
      ],
      "metadata": {
        "id": "ZpshPN5_EygY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.2: Calculate the number of observations in the id_map table associated with district 1.\n",
        "%%sql\n",
        "SELECT COUNT(*)\n",
        "FROM id_map\n",
        "WHERE district_id = 1;"
      ],
      "metadata": {
        "id": "fdcJjWOxE0qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.3: Calculate the number of observations in the id_map table associated with district 3."
      ],
      "metadata": {
        "id": "rycJOdF-E9lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT COUNT(*)\n",
        "FROM id_map\n",
        "WHERE district_id = 3;"
      ],
      "metadata": {
        "id": "6YPwugDmE93t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ask 4.5.4: Join the unique building IDs from Kavrepalanchok in id_map, all the columns from building_structure, and the damage_grade column from building_damage, limiting your results to 5 rows. Make sure you rename the building_id column in id_map as b_id and limit your results to the first five rows of the new table."
      ],
      "metadata": {
        "id": "6uGRwatHFDAS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HB1-YYiOFQ9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT distinct(i.building_id) AS b_id,\n",
        "       s.*,\n",
        "       d.damage_grade\n",
        "FROM id_map AS i\n",
        "JOIN building_structure AS s ON i.building_id = s.building_id\n",
        "JOIN building_damage AS d ON i.building_id = d.building_id\n",
        "WHERE i.district_id = 3\n",
        "LIMIT 5;\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zzi3L6VZFFR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.5: Write a wrangle function that will use the query you created in the previous task to create a DataFrame. In addition your function should:\n",
        "\n",
        "# Create a \"severe_damage\" column, where all buildings with a damage grade greater than 3 should be encoded as 1. All other buildings should be encoded at 0.\n",
        "# Drop any columns that could cause issues with leakage or multicollinearity in your model."
      ],
      "metadata": {
        "id": "3esdG0O0FJGZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "def wrangle(db_path):\n",
        "    \"\"\"\n",
        "    Wrangle data from an SQLite database to prepare it for analysis.\n",
        "\n",
        "    Parameters:\n",
        "    db_path (str): Path to the SQLite database file.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame with processed and cleaned data.\n",
        "    \"\"\"\n",
        "    # Connect to the SQLite database\n",
        "    conn = sqlite3.connect(db_path)\n",
        "\n",
        "    # Construct the SQL query\n",
        "    query = \"\"\"\n",
        "    SELECT DISTINCT(i.building_id) AS b_id,\n",
        "           s.*,\n",
        "           d.damage_grade\n",
        "    FROM id_map AS i\n",
        "    JOIN building_structure AS s ON i.building_id = s.building_id\n",
        "    JOIN building_damage AS d ON i.building_id = d.building_id\n",
        "    WHERE i.district_id = 3\n",
        "    \"\"\"\n",
        "\n",
        "    # Read query results into a DataFrame\n",
        "    df = pd.read_sql_query(query, conn, index_col='b_id')\n",
        "\n",
        "    # Clean the `damage_grade` column to extract numeric values\n",
        "    df[\"damage_grade\"] = df[\"damage_grade\"].str.extract(r'(\\d+)', expand=False).astype(float)\n",
        "\n",
        "    # Create the `severe_damage` column (binary encoding: 1 if damage_grade > 3, else 0)\n",
        "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
        "\n",
        "    # Identify columns to drop (any column containing \"post_eq\" in its name)\n",
        "    drop_cols = [col for col in df.columns if \"post_eq\" in col]\n",
        "\n",
        "    # Add additional columns to drop\n",
        "    additional_drop_cols = [\n",
        "        \"damage_grade\",         # No longer needed after creating `severe_damage`\n",
        "        \"count_floors_pre_eq\",  # Presumably unnecessary for analysis\n",
        "        \"building_id\"           # Redundant as `b_id` is already used as the index\n",
        "    ]\n",
        "    drop_cols.extend(additional_drop_cols)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Use the wrangle function\n",
        "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "ZD3uyLERFLgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "C-RP-PPYFR9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot value counts of `\"severe_damage\"`\n",
        "# create bar chart using\n",
        "# severe damage column which\n",
        "# contains two classes\n",
        "df[\"severe_damage\"].value_counts(normalize=True).plot(\n",
        "    kind=\"bar\", xlabel=\"Severe Damage\", ylabel=\"Relative Frequency\", title=\"Class Balance\"\n",
        ");\n",
        "# Don't delete the code below ðŸ‘‡\n",
        "plt.savefig(\"images/4-5-6.png\", dpi=150)\n"
      ],
      "metadata": {
        "id": "Bt-mVP9VFTmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.7: Is there a relationship between the footprint size of a building and the damage it sustained in the earthquake? Use seaborn to create a boxplot that shows the distributions of the \"plinth_area_sq_ft\" column for both groups in the \"severe_damage\" column. Label your x-axis \"Severe Damage\" and y-axis \"Plinth Area [sq. ft.]\". Use the title \"Kavrepalanchok, Plinth Area vs Building Damage\"."
      ],
      "metadata": {
        "id": "4OG6uAISFW_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# severe_damage: column with 2 groups\n",
        "# plinth_area_sq_ft: column: footprint size of building\n",
        "import seaborn as sns\n",
        "sns.boxplot(x=\"severe_damage\", y=\"plinth_area_sq_ft\", data=df)\n",
        "plt.xlabel(\"Severe Damage\")\n",
        "plt.ylabel(\"Plinth Area [sq. ft.]\")\n",
        "plt.title(\"Kavrepalanchok, Plinth Area vs Building Damage\");\n",
        "# Don't delete the code below ðŸ‘‡\n",
        "plt.savefig(\"images/4-5-7.png\", dpi=150)\n"
      ],
      "metadata": {
        "id": "Wm7MLhLiFVwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.8: Are buildings with certain roof types more likely to suffer severe damage? Create a pivot table of df where the index is \"roof_type\" and the values come from the \"severe_damage\" column, aggregated by the mean."
      ],
      "metadata": {
        "id": "n2MUfQ29FZx3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roof_pivot = pd.pivot_table(\n",
        "    df, index=\"roof_type\", values=\"severe_damage\", aggfunc=np.mean    # roof_type: column in table\n",
        ").sort_values(by=\"severe_damage\")\n",
        "roof_pivot"
      ],
      "metadata": {
        "id": "42xDHsElFbtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Task 4.5.9:** Create your feature matrix `X` and target vector `y`. Your target is `\"severe_damage\"`."
      ],
      "metadata": {
        "id": "WxCjVVheFi59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=\"severe_damage\")    # feature matrix: all columns apart from severe_damage\n",
        "y = df[\"severe_damage\"]       # target vector\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ3niL_ZFj9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.10: Divide your dataset into training and validation sets using a randomized split. Your validation set should be 20% of your data."
      ],
      "metadata": {
        "id": "Iivcn9LQFlL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X , y , test_size=0.2 , random_state=42)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)"
      ],
      "metadata": {
        "id": "wh-MPNOeFeAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.11: Calculate the baseline accuracy score for your model"
      ],
      "metadata": {
        "id": "oWWb_wwdFsgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_baseline = y_train.value_counts(normalize = True).max()\n",
        "print(\"Baseline Accuracy:\", round(acc_baseline, 2))"
      ],
      "metadata": {
        "id": "tXn4VlPUFvUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.12: Create a model model_lr that uses logistic regression to predict building damage. Be sure to include an appropriate encoder for categorical features."
      ],
      "metadata": {
        "id": "qAs7yiLFFwCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lr = make_pipeline(\n",
        "    OneHotEncoder(handle_unknown='ignore'),  # Encodes categorical features\n",
        "    LogisticRegression(max_iter=2000)        # Specify max_iter to suppress warnings\n",
        ")"
      ],
      "metadata": {
        "id": "6kHuV3aZFyYf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.13: Calculate training and validation accuracy score for model_lr"
      ],
      "metadata": {
        "id": "l2YxAlchFz08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_train_acc = accuracy_score(y_train , model_lr.predict(X_train))\n",
        "\n",
        "lr_val_acc =model_lr.score(X_val , y_val)\n",
        "\n",
        "print(\"Logistic Regression, Training Accuracy Score:\", lr_train_acc)\n",
        "print(\"Logistic Regression, Validation Accuracy Score:\", lr_val_acc)"
      ],
      "metadata": {
        "id": "xTdcobPaF6FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "14MEmVGQF7Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.14: Perhaps a decision tree model will perform better than logistic regression, but what's the best hyperparameter value for max_depth? Create a for loop to train and evaluate the model model_dt at all depths from 1 to 15. Be sure to use an appropriate encoder for your model, and to record its training and validation accuracy scores at every depth. The grader will evaluate your validation accuracy scores only."
      ],
      "metadata": {
        "id": "8CNkRaFwF7u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depth_hyperparams = range(1, 16)\n",
        "training_acc = []\n",
        "validation_acc = []\n",
        "for d in depth_hyperparams:\n",
        "    model_dt = make_pipeline(\n",
        "        OrdinalEncoder(),\n",
        "        DecisionTreeClassifier(max_depth=d , random_state =42)\n",
        "\n",
        "    )\n",
        "    model_dt.fit(X_train, y_train)\n",
        "    training_acc.append(model_dt.score(X_train , y_train))\n",
        "    validation_acc.append(model_dt.score(X_val,y_val))\n",
        ""
      ],
      "metadata": {
        "id": "S_utlgXJF_hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.15: Using the values in training_acc and validation_acc, plot the validation curve for model_dt. Label your x-axis \"Max Depth\" and your y-axis \"Accuracy Score\". Use the title \"Validation Curve, Decision Tree Model\", and include a legend"
      ],
      "metadata": {
        "id": "No4zv7rzGA63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Don't delete the code below ðŸ‘‡\n",
        "plt.savefig(\"images/4-5-15.png\", dpi=150)\n"
      ],
      "metadata": {
        "id": "IPlvdrVzGE0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.16: Build and train a new decision tree model final_model_dt, using the value for max_depth that yielded the best validation accuracy score in your plot above."
      ],
      "metadata": {
        "id": "0GiyUtqlGFdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation curve\n",
        "plt.plot(depth_hyperparams, training_acc, label=\"Training\")\n",
        "plt.plot(depth_hyperparams, validation_acc, label=\"validation\")\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.title(\"Validation Curve, Decision Tree Model\")\n",
        "plt.legend();\n",
        "\n",
        "\n",
        "# build & fit again\n",
        "final_model_dt = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    DecisionTreeClassifier(max_depth=10, random_state=42)\n",
        ")\n",
        "# Fit model to training data\n",
        "final_model_dt.fit(X, y)"
      ],
      "metadata": {
        "id": "uxL6ycJMGIBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4.5.17: How does your model perform on the test set? First, read the CSV file \"data/kavrepalanchok-test-features.csv\" into the DataFrame X_test. Next, use final_model_dt to generate a list of test predictions y_test_pred. Finally, submit your test predictions to the grader to see how your model performs.\n",
        "\n",
        "# # Tip: Make sure the order of the columns in X_test is the same as in your X_train. Otherwise, it could hurt your model's performance."
      ],
      "metadata": {
        "id": "CFqc2X9ZGJQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(\"data/kavrepalanchok-test-features.csv\", index_col=\"b_id\")\n",
        "y_test_pred = pd.Series(final_model_dt.predict(X_test))\n",
        "y_test_pred[:5]"
      ],
      "metadata": {
        "id": "jRYnQkZJGNqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}